{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the dataset from the URL into a dataframe.\n",
    "- Display the first few rows to make sure it was read properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  1      2      3      4       5        6        7       8   \\\n",
      "0    842302  M  17.99  10.38  122.8  1001.0  0.11840  0.27760  0.3001   \n",
      "1    842517  M  20.57  17.77  132.9  1326.0  0.08474  0.07864  0.0869   \n",
      "2  84300903  M  19.69  21.25  130.0  1203.0  0.10960  0.15990  0.1974   \n",
      "\n",
      "        9    ...        22     23     24      25      26      27      28  \\\n",
      "0  0.14710   ...     25.38  17.33  184.6  2019.0  0.1622  0.6656  0.7119   \n",
      "1  0.07017   ...     24.99  23.41  158.8  1956.0  0.1238  0.1866  0.2416   \n",
      "2  0.12790   ...     23.57  25.53  152.5  1709.0  0.1444  0.4245  0.4504   \n",
      "\n",
      "       29      30       31  \n",
      "0  0.2654  0.4601  0.11890  \n",
      "1  0.1860  0.2750  0.08902  \n",
      "2  0.2430  0.3613  0.08758  \n",
      "\n",
      "[3 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Reading the dataset into a pandas dataframe object\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',\n",
    "                header=None)\n",
    "# Print some data to make sure the data was properly read into the dataframe\n",
    "print df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get a sense of the size of the dataset we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#rows:  569\n",
      "#columns:  32\n"
     ]
    }
   ],
   "source": [
    "rows, columns = df.shape\n",
    "print \"#rows: \", rows\n",
    "print \"#columns: \", columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is just an ID. The second column is the actual diagnosis for the tumour - M(Malignant) and B(Benign).\n",
    "\n",
    "The column numbers 3-32 include the 30 features of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the 30 features of the dataset and the target variable to separate numpy arrays. We will get the same result in the next 2 lines if we replace ```iloc``` with ```loc```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 2:].values\n",
    "Y = df.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the class labels from their original string representation (M & B) to integers. We perform all these tasks using the preprocessing libraries available in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "print Y[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the Y array has been converted from a string label (M & B) to a integer label (1 & 0). Let us now convert the dataset into training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline with the following steps chained to each other\n",
    "- For optimal performance transform all feature values into the same scale, i.e. standardize the columns before feeding them to the classifier.\n",
    "- Compress the initial 30 dimensional data into a lower 2 dimensional space using Principal Component Analysis (PCA)\n",
    "- Apply the logistic regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the steps of the pipeline, each step is a tuple: a name and either a transformer or estimator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standard_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=2, whiten=False)), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_pipeline = Pipeline([('standard_scaler', StandardScaler()),\n",
    "                                   ('pca', PCA(n_components=2)),\n",
    "                                   ('classifier', LogisticRegression(random_state=1))])\n",
    "classification_pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "accuracy = classification_pipeline.score(X_test, Y_test)\n",
    "print \"Test Accuracy: %.3f\" % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just a single test on test data, let's do a startified k-fold cross validation on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(estimator=classification_pipeline, X=X_train, y=Y_train, cv=10, n_jobs=1)\n",
    "import numpy as np\n",
    "print \"CV Accuracy %.3f +/- %.3f\" % (np.mean(scores), np.std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
