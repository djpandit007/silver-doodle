{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the dataset from the URL into a dataframe.\n",
    "- Display the first few rows to make sure it was read properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  1      2      3      4       5        6        7       8   \\\n",
      "0    842302  M  17.99  10.38  122.8  1001.0  0.11840  0.27760  0.3001   \n",
      "1    842517  M  20.57  17.77  132.9  1326.0  0.08474  0.07864  0.0869   \n",
      "2  84300903  M  19.69  21.25  130.0  1203.0  0.10960  0.15990  0.1974   \n",
      "\n",
      "        9    ...        22     23     24      25      26      27      28  \\\n",
      "0  0.14710   ...     25.38  17.33  184.6  2019.0  0.1622  0.6656  0.7119   \n",
      "1  0.07017   ...     24.99  23.41  158.8  1956.0  0.1238  0.1866  0.2416   \n",
      "2  0.12790   ...     23.57  25.53  152.5  1709.0  0.1444  0.4245  0.4504   \n",
      "\n",
      "       29      30       31  \n",
      "0  0.2654  0.4601  0.11890  \n",
      "1  0.1860  0.2750  0.08902  \n",
      "2  0.2430  0.3613  0.08758  \n",
      "\n",
      "[3 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Reading the dataset into a pandas dataframe object\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',\n",
    "                header=None)\n",
    "# Print some data to make sure the data was properly read into the dataframe\n",
    "print df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get a sense of the size of the dataset we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#rows:  569\n",
      "#columns:  32\n"
     ]
    }
   ],
   "source": [
    "rows, columns = df.shape\n",
    "print \"#rows: \", rows\n",
    "print \"#columns: \", columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is just an ID. The second column is the actual diagnosis for the tumour - M(Malignant) and B(Benign).\n",
    "\n",
    "The column numbers 3-32 include the 30 features of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the 30 features of the dataset and the target variable to separate numpy arrays. We will get the same result in the next 2 lines if we replace ```iloc``` with ```loc```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 2:].values\n",
    "Y = df.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the class labels from their original string representation (M & B) to integers. We perform all these tasks using the preprocessing libraries available in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "print Y[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the Y array has been converted from a string label (M & B) to a integer label (1 & 0). Let us now convert the dataset into training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline with the following steps chained to each other\n",
    "- For optimal performance transform all feature values into the same scale, i.e. standardize the columns before feeding them to the classifier.\n",
    "- Compress the initial 30 dimensional data into a lower 2 dimensional space using Principal Component Analysis (PCA)\n",
    "- Apply the logistic regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the steps of the pipeline, each step is a tuple: a name and either a transformer or estimator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standard_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=2, whiten=False)), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_pipeline = Pipeline([('standard_scaler', StandardScaler()),\n",
    "                                   ('pca', PCA(n_components=2)),\n",
    "                                   ('classifier', LogisticRegression(random_state=1))])\n",
    "classification_pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "accuracy = classification_pipeline.score(X_test, Y_test)\n",
    "print \"Test Accuracy: %.3f\" % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just a single test on test data, let's do a startified k-fold cross validation on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy 0.950 +/- 0.029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(estimator=classification_pipeline, X=X_train, y=Y_train, cv=10, n_jobs=1)\n",
    "import numpy as np\n",
    "print \"CV Accuracy %.3f +/- %.3f\" % (np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now plot the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer we get is a list of lists. The first list represents the class 0 (-ve) and the second list represents the class 1 (+ve). To convert this into the convention we saw in class, stack the 2 lists, one in each row to get\n",
    "\n",
    "[71, 1] : [True positive, False positive]  \n",
    "[5, 37] : [False negative, True negative]  \n",
    "Rows are TRUE/OBSERVED CLASS. Columns are PREDICTED CLASS above.\n",
    "\n",
    "Now switch the elements on the diagonal to get  \n",
    "[37, 1] : [True positive, False positive]  \n",
    "[5, 71] : [False negative, True negative]  \n",
    "Rows are PREDICTED CLASS. Columns are TRUE/OBSERVED CLASS above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71  1]\n",
      " [ 5 37]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = classification_pipeline.predict(X_test)\n",
    "confusionMatrix = confusion_matrix(y_true=Y_test, y_pred=y_pred)\n",
    "print confusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing precision, recall and F measure. All this is a part of the classification report shown below. Classification report gives these measures for both the positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96        72\n",
      "          1       0.97      0.88      0.93        42\n",
      "\n",
      "avg / total       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print \"\", classification_report(y_true=Y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get the same statistics for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.974\n",
      "Recall: 0.881\n",
      "F1: 0.925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print \"Precision: %.3f\" % precision_score(y_true=Y_test, y_pred=y_pred)\n",
    "print \"Recall: %.3f\" % recall_score(y_true=Y_test, y_pred=y_pred)\n",
    "print \"F1: %.3f\" % f1_score(y_true=Y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the logistic regression model with different parameters and try to finetune these hyperparameters using the Grid Search approach.  \n",
    "Any parameter provided when constructing an estimator may be optimized in this manner.\n",
    "\n",
    "Specifically, to find the names and current values for all the parameters for a given estimator, use: ```estimator.get_params()```  \n",
    "\n",
    "A list of available parameters to tune can be displayed by using ```classification_pipeline.get_params().keys()```\n",
    "\n",
    "The param name is constructed as the string name given to it in the pipeline constructor followed by 2 underscores and then the param name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters available for tuning:  ['standard_scaler__copy', 'pca__whiten', 'classifier__max_iter', 'classifier__C', 'classifier__multi_class', 'standard_scaler__with_mean', 'classifier__intercept_scaling', 'classifier__warm_start', 'pca', 'classifier__class_weight', 'pca__copy', 'classifier__solver', 'classifier__dual', 'classifier__fit_intercept', 'classifier__n_jobs', 'classifier__verbose', 'pca__n_components', 'classifier__tol', 'standard_scaler__with_std', 'standard_scaler', 'steps', 'classifier', 'classifier__random_state', 'classifier__penalty']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('standard_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=2, whiten=False)), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], 'classifier__penalty': ['l1', 'l2']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "print \"Parameters available for tuning: \", classification_pipeline.get_params().keys()\n",
    "param_grid = [{\"classifier__penalty\": ['l1', 'l2'], \"classifier__C\": param_range}]\n",
    "# In the above line, each set of items enclosed in {} defines one grid. Here we only have one grid, but we could have multiple grids to explore\n",
    "gs = GridSearchCV(estimator=classification_pipeline,\n",
    "                 param_grid=param_grid,\n",
    "                 scoring='accuracy',\n",
    "                 cv=10,\n",
    "                 n_jobs=-1)\n",
    "gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy score:  0.951648351648\n",
      "Best parameters:  {'classifier__C': 1.0, 'classifier__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "print \"Best accuracy score: \", gs.best_score_\n",
    "print \"Best parameters: \", gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
